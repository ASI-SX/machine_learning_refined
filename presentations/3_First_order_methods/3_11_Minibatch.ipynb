{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chapter 3: First order methods\n",
    "\n",
    "\n",
    "# 3.11 Mini-batch optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In machine learning applications we almost never tasked with minimizing a single mathematical function, but one that consist a *sum* of $P$ functions.  \n",
    "\n",
    "\n",
    "- In other words the sort of function $g$ we very often need to minimize in machine learning applications takes the general form \n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(\\mathbf{w}\\right) = \\sum_{p=1}^P g_p\\left(\\mathbf{w}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "where $g_1,\\,g_2,\\,...,g_P$ are mathematical functions themselves.  \n",
    "\n",
    "\n",
    "- In machine learning applications hese functions $g_1,\\,g_2,\\,...,g_P$ are almost always of the same type - e.g., they can be convex quadratic functions with different constants paramterized by the same weights $\\mathbf{w}$.  \n",
    "\n",
    "\n",
    "- This special *summation structure* allows for a simple but very effective enhancement to virtually any local optimization scheme, and is called *mini-batch optimization*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- You can toggle the code on and off in this presentation via the button below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## This code cell will not be shown in the HTML version of this notebook\n",
    "# import standard tools\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import autograd.numpy as np\n",
    "import time\n",
    "\n",
    "# import custom plotting tools\n",
    "from mlrefined_libraries import math_optimization_library as optlib\n",
    "from mlrefined_libraries import calculus_library as callib\n",
    "static_plotter = optlib.static_plotter.Visualizer();\n",
    "anime_plotter = optlib.animation_plotter.Visualizer();\n",
    "\n",
    "# The next three lines are needed to compensate for matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.11.1  A simple idea with powerful consequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Suppose we were to apply a local optimization scheme to minimize a function $g$ of the form\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(\\mathbf{w}\\right) = \\sum_{p=1}^P g_p\\left(\\mathbf{w}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $g_1\\,g_2,\\,...,g_P$ are all functions of the same kind (e.g., quadratics with different constants parameterized by $\\mathbf{w}$).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- What would happen if we were to try to minimize $g$ by taking descent steps in the summand functions $g_1,\\,g_2,\\,...,g_P$ one-at-a-time?   \n",
    "\n",
    "\n",
    "- As we will see empircaly throughout this text, starting with the examples below, in many instances this idea can actually lead to considerably faster optimization of a function $g$ consisting of a sum of $P$ functions as detailed in general above above.\n",
    "\n",
    "\n",
    "- The gist of this idea is drawn graphically in the figure below for the case $P = 3$, where we compare the idea of taking a the a descent step simultaneously in $g_1,\\,g_2,\\,...,g_P$ versus a sequence of $P$ descent steps in $g_1$ then $g_2$ etc., up to $g_P$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src= '../../mlrefined_images/math_optimization_images/batch_vs_miinbatch_functions.png' width=\"80%\" height=\"auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.11.2  Descending with larger mini-batch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Instead of taking $P$ sequential steps in single functions $g_p$ (a mini-batch of *size $1$*) one-at-a-time, we can more general (with functions $g$ that take the form $g\\left(\\mathbf{w}\\right) = \\sum_{p=1}^P g_p\\left(\\mathbf{w}\\right)$) take fewer steps in one epoch, but take each step with respect to *several* of the functions $g_p$ e.g., two functions at-a-time, or three functions at-a-time, etc.,.  \n",
    "\n",
    "\n",
    "- With this slight twist on the idea detailed above we take fewer steps per epoch but take each with respect to larger non-overlapping subsets of the functions $g_1,\\,g_2,\\,...,g_P$, but still sweep through each function exactly once per epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.11.3  Mini-batch optimization general performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Is the trade-off - taking more steps per epoch with a mini-batch approach as opposed a full descent step - worth the extra effort?  Typically *yes*.  \n",
    "\n",
    "\n",
    "- Often in practice when minimizing machine learning functions an epoch of mini-batch steps like those detailed above will drastically outperform an analagous full descent step - often referred to as a *full batch* or simply a *batch* epoch in the context of mini-batch optimiztaion.  \n",
    "\n",
    "\n",
    "- A prototypical comparison of a cost function history employing a batch and corresponding epochs of mini-batch optimization applied to the same hypothetical function $g$ with the same initialization $\\mathbf{w}^0$ is shown in the figure below.  \n",
    "\n",
    "\n",
    "- Because we take far more steps with the mini-batch approach and because each $g_p$ takes the same form, each epoch of the mini-batch approach typically outperforms its full batch analog.  \n",
    "\n",
    "\n",
    "- Even when taking into account that far more descent steps are taken during an epoch of mini-batch optimization the method often greatly outperforms its full batch analog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src= '../../mlrefined_images/math_optimization_images/minibatch_functions.png' width=\"80%\" height=\"auto\"/>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "141.067px",
    "width": "251.1px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
